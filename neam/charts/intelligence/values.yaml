# Default values for intelligence service
# This file uses the shared library chart for common configurations

global:
  labels:
    app.kubernetes.io/part-of: neam-platform

# Override chart name
nameOverride: "intelligence"
fullnameOverride: "neam-intelligence"

# Replica configuration
replicaCount: 3

# Image configuration
image:
  repository: neam-platform/intelligence
  pullPolicy: IfNotPresent
  tag: "1.0.0"

# Service account
serviceAccount:
  create: true
  name: "neam-intelligence"
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::123456789:role/neam-intelligence-role"

# Deployment configuration
deployment:
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"

# Pod configuration
podAnnotations:
  checksum/config: "mno345"
  sidecar.istio.io/inject: "true"

podSecurityContext:
  enabled: true
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

# Container configuration
containerPorts:
  http: 8080
  grpc: 9090
  metrics: 9090

env:
  - name: LOG_LEVEL
    value: "info"
  - name: LOG_FORMAT
    value: "json"
  - name: OTEL_SERVICE_NAME
    value: "neam-intelligence"

envFrom:
  - secretRef:
      name: intelligence-secrets
  - configMapRef:
      name: intelligence-config

command:
  - "/intelligence"

volumeMounts:
  - name: config
    mountPath: /etc/intelligence
    readOnly: true
  - name: models
    mountPath: /models

volumes:
  - name: config
    configMap:
      name: intelligence-config
  - name: models
    persistentVolumeClaim:
      claimName: intelligence-models

# Security context
containerSecurityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL

# Health probes
livenessProbe:
  enabled: true
  path: /health/live
  port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

readinessProbe:
  enabled: true
  path: /health/ready
  port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3
  successThreshold: 1

startupProbe:
  enabled: true
  path: /health/startup
  port: 8080
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 30
  failureThreshold: 60

# Resources
resources:
  limits:
    cpu: "8"
    memory: 16Gi
    nvidia.com/gpu: 1
  requests:
    cpu: "2"
    memory: 4Gi
    nvidia.com/gpu: 1

# Networking
service:
  enabled: true
  type: ClusterIP
  port: 8080
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"

serviceHeadless:
  enabled: false

ingress:
  enabled: false

# Configuration
configmap:
  create: true
  data:
    config.yaml: |
      server:
        host: "0.0.0.0"
        port: 8080
        grpc_port: 9090
      
      models:
        default: llm-model
        registry:
          llm-model:
            path: /models/llm
            version: 1.0.0
            max_tokens: 4096
            temperature: 0.7
      
      inference:
        batch_size: 32
        max_concurrent: 100
        timeout: 30s
      
      cache:
        redis_url: redis://redis:6379
        ttl: 1h
      
      gpu:
        enabled: true
        device: nvidia

configmapFromFile:
  create: false

secret:
  create: false

tlsSecret:
  create: false

# Storage
persistentVolumeClaim:
  create: true
  size: 100Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: gp3
  annotations:
    helm.sh/resource-policy: keep

# Advanced configuration
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: intelligence

nodeSelector:
  node-type: gpu

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - intelligence
          topologyKey: kubernetes.io/hostname
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu
                operator: Exists

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

terminationGracePeriodSeconds: 60

priorityClassName: critical-priority

# Autoscaling
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

# Pod disruption budget
podDisruptionBudget:
  enabled: true
  minAvailable: 2

# Image pull secrets
imagePullSecrets:
  - name: registry-credentials
